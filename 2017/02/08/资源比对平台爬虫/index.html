<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 资源比对平台爬虫 · Bonjour!</title><meta name="description" content="资源比对平台爬虫 - BonjourChen"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Bonjour!"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/webinchen" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/bonjourchen" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">资源比对平台爬虫</h1><div class="post-info">Feb 8, 2017</div><div class="post-content"><p>话说我司有个资源比对平台，每周需要从上面下载资源比对数据。每次都需要登录进去点来点去才能下载很麻烦。于是就想写个爬虫可以自动从上面下载。</p>
<blockquote>
<p>PS：这个平台是在我司内网里的。外网无法访问。各位看看就好，主要是思路。</p>
</blockquote>
<p>爬虫用的框架是著名的Scrapy。（感觉有点大材小用了）<br><a id="more"></a><br>很简单先创建一个Scrapy工程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">(py3) λ scrapy startproject biduicrawl</div><div class="line">New Scrapy project &apos;biduicrawl&apos;, using template directory &apos;d:\\anaconda2\\envs\\py3\\lib\\site-packages\\scrapy\\templates\\project&apos;, created in:</div><div class="line">    E:\biduicrawl</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd biduicrawl</div><div class="line">    scrapy genspider example example.com</div></pre></td></tr></table></figure></p>
<p>创建完了项目之后，目录树如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">E:.</div><div class="line">│  scrapy.cfg</div><div class="line">└─biduicrawl</div><div class="line">    │  items.py</div><div class="line">    │  middlewares.py</div><div class="line">    │  pipelines.py</div><div class="line">    │  settings.py</div><div class="line">    │  __init__.py</div><div class="line">    ├─spiders</div><div class="line">    │  │  __init__.py</div><div class="line">    │  └─__pycache__</div><div class="line">    └─__pycache__</div></pre></td></tr></table></figure></p>
<p>我们在<code>spiders</code>文件夹里面加一个文件<code>biduipingtai.py</code>，搞起！</p>
<h4 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h4><p>其实爬虫的难点不在于代码，而在于怎么抓到想要的数据。要理清提交什么请求才能得到正确的响应，具体的实现Scrapy都给我们做好了。<br>浏览器打开网址，再按F12，切到Network的选项。先解决怎么登进去的问题。<br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/112521518.png" alt="mark"><br>输入账号密码图片验证码之后，就进去了。这时候就要找到底账号密码的请求是怎么POST过去的。<br>这里我们最好勾一下Network下面的Preserve log，这个是保存之前的log，避免网页跳转的时候把之前的log给清除掉。<br>输入账号和密码的时候看到下面面板上显示了一个请求。打开看到了一串Request Payload，上面有密码：<br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/112652286.png" alt="mark"><br>但是账号呢！账号呢！看来就不是这里了。接着找。我们登进去看看：<br>一个个找，终于找到了提交表单的位置<br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/112742431.png" alt="mark"><br>原来这么简单的！给这个URL去POST一个Form Data就可以了，验证码都是不需要的咯。<br>我们用Firefox验证一下，用HackBar插件：<br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/112346873.png" alt="mark"><br>进来了。。。<br>就可以开始写代码了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiduiSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'biduipingtai_spider'</span></div><div class="line">    allowed_domains = [<span class="string">"http://132.121.80.158:8090/"</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        url = <span class="string">'http://132.121.80.158:8090/plversion/security_check'</span></div><div class="line">        headers = &#123;</div><div class="line">            <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'</span>,</div><div class="line">            <span class="string">'Host'</span>: <span class="string">'132.121.80.158:8090'</span>,</div><div class="line">            <span class="string">'Origin'</span>: <span class="string">'http://132.121.80.158:8090'</span>,</div><div class="line">            <span class="string">'Referer'</span>: <span class="string">'http://132.121.80.158:8090/plversion/com.gxlu.security.view.login.d'</span>,</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>,</div><div class="line">            <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span></div><div class="line">        &#125;</div><div class="line">        login_info = &#123;</div><div class="line">            <span class="string">'PL'</span>: <span class="string">'1'</span>,</div><div class="line">            <span class="string">'password'</span>: <span class="string">'******'</span>,</div><div class="line">            <span class="string">'username'</span>: <span class="string">'******'</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">yield</span> scrapy.FormRequest(</div><div class="line">            url=url,</div><div class="line">            method=<span class="string">'POST'</span>,</div><div class="line">            headers=headers,</div><div class="line">            formdata=login_info,</div><div class="line">            callback=self.parse_device,</div><div class="line">            dont_filter=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<h4 id="下载文件"><a href="#下载文件" class="headerlink" title="下载文件"></a>下载文件</h4><p>接着找下载文件的请求。我们一路点到下载文件的地方，然后把文件下载下来，一个请求一个请求慢慢看。最后我们看到:<br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/111708217.png" alt="mark"><br><img src="http://of9x0sxb3.bkt.clouddn.com/blog/20170208/112003961.png" alt="mark"><br>这个json里面的data就是要下载的文件名了。然后结合刚刚下载的文件的<code>url：http://132.121.80.158:8090/plversion/excel/1486523745046.zip</code>,于是可以构造下载url了。<br>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获取下载设备比对结果的路径</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_device</span><span class="params">(self, response)</span>:</span></div><div class="line">    url = <span class="string">'http://132.121.80.158:8090/plversion/dorado/view-service'</span></div><div class="line">    body = <span class="string">'&#123;"action":"remote-service","service":"outputAllDataService#findAllDataOutPutExcel","parameter":&#123;"pageflag":"TMP_IPRANNETL2WGDIFFERENT","pageNo":1,"pageSize":60000&#125;,"context":&#123;"DataORG_ID":null,"orgId":null,"pageflag":"TMP_IPRANNETL2WGDIFFERENT","ListStatisSQL":null,"CompareStatus":null,"Createdate":null,"viewId":"com.gxlu.statisticommon.view.charttest"&#125;,"loadedDataTypes":["dataTypeGisProject","dataTypeDatatable","dataTypeTelant","dataTypeOrgStruct","dataTypeTime"]&#125;'</span></div><div class="line">    headers = &#123;</div><div class="line">        <span class="comment"># 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',</span></div><div class="line">        <span class="comment"># 'Host': '132.121.80.158:8090',</span></div><div class="line">        <span class="string">'Origin'</span>: <span class="string">'http://132.121.80.158:8090'</span>,</div><div class="line">        <span class="string">'Referer'</span>: <span class="string">'http://132.121.80.158:8090/plversion/com.gxlu.statisticommon.view.ListConfig.d?viewId=com.gxlu.statisticommon.view.charttest&amp;pageflag=TMP_IPRANNETL2WGDIFFERENT&amp;wordUri=4GdataCompare'</span>,</div><div class="line">        <span class="string">'Content-Type'</span>: <span class="string">'text/javascript'</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">yield</span> scrapy.Request(</div><div class="line">        url=url,</div><div class="line">        method=<span class="string">'POST'</span>,</div><div class="line">        body=body,</div><div class="line">        headers=headers,</div><div class="line">        callback=self.parse_device_download,</div><div class="line">        dont_filter=<span class="keyword">True</span></div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># 获取下载文件名</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_device_download</span><span class="params">(self, response)</span>:</span></div><div class="line">    text = response.text</div><div class="line">    text = json.loads(text)</div><div class="line">    filename = text[<span class="string">'data'</span>]</div><div class="line">    url = <span class="string">'http://132.121.80.158:8090/plversion/excel/'</span> + filename</div><div class="line">    <span class="keyword">yield</span> scrapy.Request(</div><div class="line">        url=url,</div><div class="line">        method=<span class="string">'GET'</span>,</div><div class="line">        callback=self.parse_card,</div><div class="line">        dont_filter=<span class="keyword">True</span></div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># 下载设备比对结果，获取下载板卡比对结果的路径</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_card</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># inspect_response(response, self)</span></div><div class="line">    filename_device = os.path.join(os.path.abspath(<span class="string">'.'</span>), <span class="string">'设备.zip'</span>)</div><div class="line">    <span class="keyword">with</span> open(filename_device, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(response.body)</div></pre></td></tr></table></figure></p>
<p>一个简单的爬虫就做完了。</p>
<h4 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h4><p>下载下来的文件是个zip，zip里面有多个excel，进一步操作就是将这些excel合并成一个。<br>解压：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unzip</span><span class="params">(self, filename)</span>:</span></div><div class="line">    f = ZipFile(filename)</div><div class="line">    name_list = f.namelist()</div><div class="line">    f.extractall()</div><div class="line">    <span class="keyword">return</span> name_list</div></pre></td></tr></table></figure></p>
<p>合并：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xlrd</div><div class="line"><span class="keyword">import</span> openpyxl</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> openpyxl <span class="keyword">import</span> Workbook</div><div class="line"><span class="keyword">from</span> openpyxl <span class="keyword">import</span> load_workbook</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">combination</span><span class="params">(self, filelist, filename_new)</span>:</span></div><div class="line">    ILLEGAL_CHARACTERS_RE = re.compile(</div><div class="line">        <span class="string">r'[\000-\010]|[\013-\014]|[\016-\037]'</span>)</div><div class="line">    num_of_file = len(filelist)</div><div class="line">    <span class="keyword">if</span> num_of_file == <span class="number">1</span>:</div><div class="line">        os.rename(filelist[<span class="number">0</span>], filename_new + <span class="string">'.xls'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        wb = Workbook()</div><div class="line">        ws = wb.active</div><div class="line">        count = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> filelist:</div><div class="line">            print(<span class="string">'正在读取'</span> + str(file) + <span class="string">'...'</span>)</div><div class="line">            data = xlrd.open_workbook(file)</div><div class="line">            print(<span class="string">'读取完毕！'</span>)</div><div class="line">            table = data.sheet_by_index(<span class="number">0</span>)</div><div class="line">            nrows = table.nrows</div><div class="line"></div><div class="line">            ws.append(table.row_values(<span class="number">0</span>))</div><div class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">1</span>, nrows):</div><div class="line">                <span class="keyword">try</span>:</div><div class="line">                    ws.append(table.row_values(row))</div><div class="line">                    count += <span class="number">1</span></div><div class="line">                    print(<span class="string">'已经复制'</span> + str(count) + <span class="string">'条数据！'</span>)</div><div class="line">                <span class="keyword">except</span> openpyxl.utils.exceptions.IllegalCharacterError:</div><div class="line">                    tmp = table.row_values(row)</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(tmp)):</div><div class="line">                        tmp[i] = ILLEGAL_CHARACTERS_RE.sub(<span class="string">r''</span>, tmp[i])</div><div class="line">                    ws.append(tmp)</div><div class="line">                    count += <span class="number">1</span></div><div class="line">                    print(<span class="string">'已经复制'</span> + str(count) + <span class="string">'条数据！'</span>)</div><div class="line">        wb.save(filename_new + <span class="string">'.xlsx'</span>)</div></pre></td></tr></table></figure></p>
<p>如果要处理xls，就要用<code>xlrd</code>和<code>xlwt</code>。如果要处理xlsx，就要用<code>openpyxl</code>。openpyxl并不能处理旧版的xls，这就是要用两个库的原因。<br>另外，如果excel的内容包含了不可用的ascii字符，openpyxl在处理时候会报<code>IllegalCharacterError</code>。解决办法就是将这些非法字符替换掉。</p>
<p>完整代码在：<a href="https://github.com/BonjourChen/crawl-for-biduipingtai.git" target="_blank" rel="external">Github</a></p>
<p>补充一个用requests库的写法，简单粗暴：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!-*-coding:utf-8-*-</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_file</span><span class="params">(filename_new, request_payload)</span>:</span></div><div class="line">    url = <span class="string">'http://132.121.80.158:8090/plversion/dorado/view-service'</span></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'</span>,</div><div class="line">        <span class="string">'Host'</span>: <span class="string">'132.121.80.158:8090'</span>,</div><div class="line">        <span class="string">'Origin'</span>: <span class="string">'http://132.121.80.158:8090'</span>,</div><div class="line">        <span class="string">'Referer'</span>: <span class="string">'http://132.121.80.158:8090/plversion/com.gxlu.statisticommon.view.ListConfig.d?viewId=com.gxlu.statisticommon.view.charttest&amp;pageflag=TMP_IPRANNETL2WGDIFFERENT&amp;wordUri=4GdataCompare'</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>,</div><div class="line">        <span class="string">'Content-Type'</span>: <span class="string">'text/javascript'</span></div><div class="line">    &#125;</div><div class="line">    response = requests.post(url=url, headers=headers, data=request_payload)</div><div class="line">    filename = json.loads(response.text)[<span class="string">'data'</span>]</div><div class="line">    url_file = <span class="string">'http://132.121.80.158:8090/plversion/excel/'</span> + filename</div><div class="line">    print(<span class="string">'正在下载'</span> + filename_new + <span class="string">'...'</span>)</div><div class="line">    file_get = requests.get(url_file)</div><div class="line">    <span class="keyword">with</span> open(filename_new, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(file_get.content)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    url = <span class="string">'http://132.121.80.158:8090/plversion/security_check'</span></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'</span>,</div><div class="line">        <span class="string">'Host'</span>: <span class="string">'132.121.80.158:8090'</span>,</div><div class="line">        <span class="string">'Origin'</span>: <span class="string">'http://132.121.80.158:8090'</span>,</div><div class="line">        <span class="string">'Referer'</span>: <span class="string">'http://132.121.80.158:8090/plversion/com.gxlu.security.view.login.d'</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</span>,</div><div class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    login_info = &#123;</div><div class="line">        <span class="string">'PL'</span>: <span class="string">'1'</span>,</div><div class="line">        <span class="string">'password'</span>: <span class="string">'******'</span>,</div><div class="line">        <span class="string">'username'</span>: <span class="string">'******'</span></div><div class="line">    &#125;</div><div class="line">    r = requests.post(url=url, headers=headers, data=login_info)</div><div class="line"></div><div class="line">    request_payload_device = <span class="string">'&#123;"action":"remote-service","service":"outputAllDataService#findAllDataOutPutExcel","parameter":&#123;"pageflag":"TMP_IPRANNETL2WGDIFFERENT","pageNo":1,"pageSize":60000&#125;,"context":&#123;"orgId":null,"DataORG_ID":null,"pageflag":"TMP_IPRANNETL2WGDIFFERENT","ListStatisSQL":null,"Createdate":null,"CompareStatus":null,"viewId":"com.gxlu.statisticommon.view.charttest"&#125;,"loadedDataTypes":["dataTypeTelant","dataTypeDatatable","dataTypeGisProject","dataTypeTime","dataTypeOrgStruct"]&#125;'</span></div><div class="line">    request_payload_card = <span class="string">'&#123;"action":"remote-service","service":"outputAllDataService#findAllDataOutPutExcel","parameter":&#123;"pageflag":"TMP_IPRANCARDTL2WGDIFFERENT","pageNo":3,"pageSize":60000&#125;,"context":&#123;"orgId":null,"DataORG_ID":null,"pageflag":"TMP_IPRANCARDTL2WGDIFFERENT","ListStatisSQL":null,"Createdate":null,"CompareStatus":null,"viewId":"com.gxlu.statisticommon.view.charttest"&#125;,"loadedDataTypes":["dataTypeTime","dataTypeDatatable","dataTypeTelant","dataTypeOrgStruct","dataTypeGisProject"]&#125;'</span></div><div class="line">    request_payload_circuit = <span class="string">'&#123;"action":"remote-service","service":"outputAllDataService#findAllDataOutPutExcel","parameter":&#123;"pageflag":"TMP_IPRANLINKTL2WGDIFFERENT","pageNo":2,"pageSize":60000&#125;,"context":&#123;"orgId":null,"DataORG_ID":null,"pageflag":"TMP_IPRANLINKTL2WGDIFFERENT","ListStatisSQL":null,"Createdate":null,"CompareStatus":null,"viewId":"com.gxlu.statisticommon.view.charttest"&#125;,"loadedDataTypes":["dataTypeOrgStruct","dataTypeDatatable","dataTypeTime","dataTypeTelant","dataTypeGisProject"]&#125;'</span></div><div class="line"></div><div class="line">    download_file(<span class="string">'设备.zip'</span>, request_payload_device)</div><div class="line">    download_file(<span class="string">'板卡.zip'</span>, request_payload_card)</div><div class="line">    download_file(<span class="string">'电路.zip'</span>, request_payload_circuit)</div></pre></td></tr></table></figure></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/02/04/搞事情-3/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2017 <a href="http://yoursite.com">BonjourChen</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>